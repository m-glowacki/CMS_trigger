{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a92fd4e2",
   "metadata": {},
   "source": [
    "### Quantization Aware Training With Pruning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76876c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import zipfile\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a3bc3ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "data = np.load('./data_hh4b_20x12_160000.npz')\n",
    "train_X = data['train_X']      #data for training the quantized model\n",
    "train_y = data['train_y']      #data labels\n",
    "test_X = data['test_X']\n",
    "test_y = data['test_y']\n",
    "test_X_hw_hh4b = data['test_X_hw_hh4b']\n",
    "test_y_hw_hh4b = data['test_y_hw_hh4b']\n",
    "test_X_hw_snu = data['test_X_hw_snu']\n",
    "test_y_hw_snu = data['test_y_hw_snu']\n",
    "data = 0\n",
    "\n",
    "model = tf.keras.models.load_model('pruning_models/unpruned_train_1_test.h5')   # unpruned modelmodel\n",
    "stripped_pruned_model = tf.keras.models.load_model('pruning_models/pruned_train_1_test.h5') #pruned model - pruning stripped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e82d0b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compiler(model_name):\n",
    "    opt = tf.keras.optimizers.Adam(0.001)\n",
    "    sensitivity_metric = tf.keras.metrics.SensitivityAtSpecificity(name='sens_at_spec',\n",
    "                                                                             specificity=0.99925,     \n",
    "                                                                             num_thresholds=20000)     \n",
    "    auc_metric = tf.keras.metrics.AUC(name='auc', num_thresholds=200)   \n",
    "    metrics = ['accuracy', sensitivity_metric, auc_metric]\n",
    "\n",
    "    model_name.compile(optimizer=opt, loss='binary_crossentropy', metrics=metrics)\n",
    "    \n",
    "    return model_name\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def trainer(model_name, train, savename, stripping):   #function for training and saving models\n",
    "    if train:\n",
    "        opt = tf.keras.optimizers.Adam(0.001)\n",
    "        log_dir = tempfile.mkdtemp()\n",
    "        sensitivity_metric = tf.keras.metrics.SensitivityAtSpecificity(name='sens_at_spec',\n",
    "                                                                                 specificity=0.99925,     \n",
    "                                                                                 num_thresholds=20000)     \n",
    "        auc_metric = tf.keras.metrics.AUC(name='auc', num_thresholds=200)   \n",
    "        metrics = ['accuracy', sensitivity_metric, auc_metric]\n",
    "\n",
    "        model_name.compile(optimizer=opt, loss='binary_crossentropy', metrics=metrics)    \n",
    "\n",
    "        model_name.fit(train_X, \n",
    "                   train_y, \n",
    "                   epochs=50, \n",
    "                   verbose=1,\n",
    "                   batch_size=512, \n",
    "                   validation_split=.2,   \n",
    "                   shuffle=True,\n",
    "                   callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                                 patience=5,\n",
    "                                                                 restore_best_weights=True),     #What does callbacks do?\n",
    "                                pruning_callbacks.UpdatePruningStep(),\n",
    "                                tfmot.sparsity.keras.PruningSummaries(log_dir=log_dir)])\n",
    "            # Save the model again but with the pruning 'stripped' to use the regular layer types\n",
    "        if stripping:\n",
    "            model_stripped = strip_pruning(model_name)\n",
    "            model_stripped.save(savename)\n",
    "        else:\n",
    "            model_name.save(savename)\n",
    "    else:\n",
    "        model_name = load_model(savename)\n",
    "        \n",
    "\n",
    "def print_model_weights_sparsity(model):\n",
    "\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, tf.keras.layers.Wrapper):\n",
    "            weights = layer.trainable_weights\n",
    "        else:\n",
    "            weights = layer.weights\n",
    "        for weight in weights:\n",
    "            # ignore auxiliary quantization weights\n",
    "            if \"quantize_layer\" in weight.name:\n",
    "                continue\n",
    "            weight_size = weight.numpy().size\n",
    "            zero_num = np.count_nonzero(weight == 0)\n",
    "            print(\n",
    "                f\"{weight.name}: {zero_num/weight_size:.2%} sparsity \",\n",
    "                f\"({zero_num}/{weight_size})\",\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a03b876d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.sequential.Sequential at 0x7f0e63374a00>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print_model_weights_sparsity(model)\n",
    "#print_model_weights_sparsity(pruned_model)\n",
    "compiler(model)\n",
    "compiler(stripped_pruned_model)\n",
    "#pruned_model.evaluate(test_X, test_y)\n",
    "#stripped_pruned_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bd8095ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qkeras import *\n",
    "import hls4ml\n",
    "from hls4ml.model.profiling import numerical, activations_keras, boxplot\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [10,10]\n",
    "plt.rcParams['font.size'] = 16.0\n",
    "\n",
    "seed = 48\n",
    "\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# will need to clone https://github.com/kpidgeon/cms-l1-triggers for a few\n",
    "# helper functions if running notebook and include path to repo here\n",
    "sys.path.append('/usersc/bz18310/previous_notebook/cms-l1-triggers')\n",
    "\n",
    "from utils.analysis import eff_rate, optimal_eff_rate\n",
    "from utils.preprocessing import resize\n",
    "from utils.plotting import *\n",
    "from utils.hls4ml_helpers import *\n",
    "\n",
    "plt.rc('figure', figsize=(8,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "adb7bcd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1/kernel:0: 50.00% sparsity  (18/36)\n",
      "batch_normalization/gamma:0: 0.00% sparsity  (0/4)\n",
      "batch_normalization/beta:0: 0.00% sparsity  (0/4)\n",
      "batch_normalization/moving_mean:0: 0.00% sparsity  (0/4)\n",
      "batch_normalization/moving_variance:0: 0.00% sparsity  (0/4)\n",
      "conv2/kernel:0: 50.00% sparsity  (144/288)\n",
      "batch_normalization_1/gamma:0: 0.00% sparsity  (0/8)\n",
      "batch_normalization_1/beta:0: 0.00% sparsity  (0/8)\n",
      "batch_normalization_1/moving_mean:0: 0.00% sparsity  (0/8)\n",
      "batch_normalization_1/moving_variance:0: 0.00% sparsity  (0/8)\n",
      "dense1/kernel:0: 50.00% sparsity  (288/576)\n",
      "batch_normalization_2/gamma:0: 0.00% sparsity  (0/24)\n",
      "batch_normalization_2/beta:0: 0.00% sparsity  (0/24)\n",
      "batch_normalization_2/moving_mean:0: 0.00% sparsity  (0/24)\n",
      "batch_normalization_2/moving_variance:0: 0.00% sparsity  (0/24)\n",
      "output/kernel:0: 50.00% sparsity  (12/24)\n",
      "output/bias:0: 0.00% sparsity  (0/1)\n",
      "batch_normalization_3/gamma:0: 0.00% sparsity  (0/1)\n",
      "batch_normalization_3/beta:0: 0.00% sparsity  (0/1)\n",
      "batch_normalization_3/moving_mean:0: 0.00% sparsity  (0/1)\n",
      "batch_normalization_3/moving_variance:0: 0.00% sparsity  (0/1)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1 (QConv2D)              (None, 18, 10, 4)         36        \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 9, 5, 4)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 9, 5, 4)           16        \n",
      "_________________________________________________________________\n",
      "relu_c1 (QActivation)        (None, 9, 5, 4)           0         \n",
      "_________________________________________________________________\n",
      "conv2 (QConv2D)              (None, 7, 3, 8)           288       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 3, 1, 8)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 3, 1, 8)           32        \n",
      "_________________________________________________________________\n",
      "relu_c2 (QActivation)        (None, 3, 1, 8)           0         \n",
      "_________________________________________________________________\n",
      "inputFlat (Flatten)          (None, 24)                0         \n",
      "_________________________________________________________________\n",
      "dense1 (QDense)              (None, 24)                576       \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 24)                96        \n",
      "_________________________________________________________________\n",
      "relu1 (QActivation)          (None, 24)                0         \n",
      "_________________________________________________________________\n",
      "output (QDense)              (None, 1)                 25        \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 1)                 4         \n",
      "_________________________________________________________________\n",
      "sigmoid (Activation)         (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 1,073\n",
      "Trainable params: 999\n",
      "Non-trainable params: 74\n",
      "_________________________________________________________________\n",
      "conv1/kernel:0: 0.00% sparsity  (0/36)\n",
      "batch_normalization/gamma:0: 0.00% sparsity  (0/4)\n",
      "batch_normalization/beta:0: 100.00% sparsity  (4/4)\n",
      "batch_normalization/moving_mean:0: 100.00% sparsity  (4/4)\n",
      "batch_normalization/moving_variance:0: 0.00% sparsity  (0/4)\n",
      "conv2/kernel:0: 0.00% sparsity  (0/288)\n",
      "batch_normalization_1/gamma:0: 0.00% sparsity  (0/8)\n",
      "batch_normalization_1/beta:0: 100.00% sparsity  (8/8)\n",
      "batch_normalization_1/moving_mean:0: 100.00% sparsity  (8/8)\n",
      "batch_normalization_1/moving_variance:0: 0.00% sparsity  (0/8)\n",
      "dense1/kernel:0: 0.00% sparsity  (0/576)\n",
      "batch_normalization_2/gamma:0: 0.00% sparsity  (0/24)\n",
      "batch_normalization_2/beta:0: 100.00% sparsity  (24/24)\n",
      "batch_normalization_2/moving_mean:0: 100.00% sparsity  (24/24)\n",
      "batch_normalization_2/moving_variance:0: 0.00% sparsity  (0/24)\n",
      "output/kernel:0: 0.00% sparsity  (0/24)\n",
      "output/bias:0: 100.00% sparsity  (1/1)\n",
      "batch_normalization_3/gamma:0: 0.00% sparsity  (0/1)\n",
      "batch_normalization_3/beta:0: 100.00% sparsity  (1/1)\n",
      "batch_normalization_3/moving_mean:0: 100.00% sparsity  (1/1)\n",
      "batch_normalization_3/moving_variance:0: 0.00% sparsity  (0/1)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "prune_low_magnitude_conv1 (P (None, 18, 10, 4)         74        \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_max_pool (None, 9, 5, 4)           1         \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_batch_no (None, 9, 5, 4)           17        \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_relu_c1  (None, 9, 5, 4)           1         \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_conv2 (P (None, 7, 3, 8)           578       \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_max_pool (None, 3, 1, 8)           1         \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_batch_no (None, 3, 1, 8)           33        \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_relu_c2  (None, 3, 1, 8)           1         \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_inputFla (None, 24)                1         \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_dense1 ( (None, 24)                1154      \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_batch_no (None, 24)                97        \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_relu1 (P (None, 24)                1         \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_output ( (None, 1)                 51        \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_batch_no (None, 1)                 5         \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_sigmoid  (None, 1)                 1         \n",
      "=================================================================\n",
      "Total params: 2,016\n",
      "Trainable params: 999\n",
      "Non-trainable params: 1,017\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "b = 5\n",
    "config = {\n",
    "\n",
    "        'QConv2D': {\n",
    "          \"kernel_quantizer\": f'quantized_bits({b})',\n",
    "          \"bias_quantizer\": f'quantized_bits({b})'\n",
    "        },\n",
    "        'QDense': {\n",
    "          \"kernel_quantizer\": f'quantized_bits({b})',\n",
    "          \"bias_quantizer\": f'quantized_bits({b})'\n",
    "        },\n",
    "        'QActivation': {'relu': f'quantized_relu({b})'}\n",
    "\n",
    "    }\n",
    "\n",
    "model_for_quantization = tf.keras.models.load_model('pruning_models/pruned_train_1_test.h5')\n",
    "print_model_weights_sparsity(model_for_quantization)\n",
    "qmodel = utils.model_quantize(model_for_quantization, config, b)\n",
    "qmodel.summary()\n",
    "print_model_weights_sparsity(qmodel)\n",
    "compiler(qmodel)\n",
    "\n",
    "pruning_params = {\n",
    "      'pruning_schedule': tfmot.sparsity.keras.ConstantSparsity(0.5, begin_step=0, frequency=100)\n",
    "  }\n",
    "\n",
    "callbacks = [\n",
    "  tfmot.sparsity.keras.UpdatePruningStep()\n",
    "]\n",
    "\n",
    "pruned_qmodel = tfmot.sparsity.keras.prune_low_magnitude(qmodel, **pruning_params)\n",
    "\n",
    "pruned_qmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9d177320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_30\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "prune_low_magnitude_q_conv2d (None, 18, 10, 4)         78        \n",
      "_________________________________________________________________\n",
      "module_wrapper_34 (ModuleWra (None, 9, 5, 4)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 9, 5, 4)           36        \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_q_conv2d (None, 7, 3, 8)           586       \n",
      "=================================================================\n",
      "Total params: 700\n",
      "Trainable params: 354\n",
      "Non-trainable params: 346\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.core import Flatten\n",
    "from keras.layers.convolutional import *\n",
    "from keras.layers.pooling import *\n",
    "\n",
    "\n",
    "\n",
    "m = tf.keras.Sequential([\n",
    "    tfmot.sparsity.keras.prune_low_magnitude(\n",
    "    QConv2D(4, kernel_size = (3,3), activation='relu', kernel_quantizer=quantized_bits(5), bias_quantizer=quantized_bits(5)),\n",
    "        input_shape=(20,12,1) ,**pruning_params),\n",
    "    MaxPooling2D(pool_size=(2,2), padding='valid'),\n",
    "    BatchNormalization(axis=1),\n",
    "    tfmot.sparsity.keras.prune_low_magnitude(\n",
    "    QConv2D(8, kernel_size = (3,3), activation='relu', kernel_quantizer=quantized_bits(5), bias_quantizer=quantized_bits(5)),\n",
    "     **pruning_params),\n",
    "\n",
    "\n",
    "\n",
    "])\n",
    "\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aef894c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
