{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eca5260e",
   "metadata": {},
   "source": [
    "### Io type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f755b6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split        #split data for training and testing\n",
    "from sklearn.metrics import accuracy_score                  #to view accuracy\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import array_ops, math_ops      #for math operations division_no_nan\n",
    "from tensorflow.keras.layers import *\n",
    "import keras_tuner as kt\n",
    "from keras_tuner.tuners import RandomSearch, Hyperband, BayesianOptimization\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import tensorflow_model_optimization as tfmot\n",
    "from tensorflow_model_optimization.python.core.sparsity.keras import prune, pruning_callbacks, pruning_schedule\n",
    "from tensorflow_model_optimization.sparsity.keras import strip_pruning\n",
    "\n",
    "from qkeras import *\n",
    "import hls4ml\n",
    "from hls4ml.model.profiling import numerical, activations_keras, boxplot\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [10,10]\n",
    "plt.rcParams['font.size'] = 16.0\n",
    "\n",
    "seed = 48\n",
    "\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# will need to clone https://github.com/kpidgeon/cms-l1-triggers for a few\n",
    "# helper functions if running notebook and include path to repo here\n",
    "sys.path.append('/usersc/bz18310/previous_notebook/cms-l1-triggers')\n",
    "\n",
    "from utils.analysis import eff_rate, optimal_eff_rate\n",
    "from utils.preprocessing import resize\n",
    "from utils.plotting import *\n",
    "from utils.hls4ml_helpers import *\n",
    "\n",
    "plt.rc('figure', figsize=(8,6))\n",
    "\n",
    "\n",
    "###for loading and saving models\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from qkeras.utils import model_save_quantized_weights\n",
    "from qkeras.utils import load_qmodel\n",
    "\n",
    "###For Pruning summaries\n",
    "\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b20d5142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1 (Conv2D)               (None, 18, 10, 4)         36        \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 9, 5, 4)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 9, 5, 4)           16        \n",
      "_________________________________________________________________\n",
      "relu_c1 (Activation)         (None, 9, 5, 4)           0         \n",
      "_________________________________________________________________\n",
      "conv2 (Conv2D)               (None, 7, 3, 8)           288       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 3, 1, 8)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 3, 1, 8)           32        \n",
      "_________________________________________________________________\n",
      "relu_c2 (Activation)         (None, 3, 1, 8)           0         \n",
      "_________________________________________________________________\n",
      "inputFlat (Flatten)          (None, 24)                0         \n",
      "_________________________________________________________________\n",
      "dense1 (Dense)               (None, 24)                576       \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 24)                96        \n",
      "_________________________________________________________________\n",
      "relu1 (Activation)           (None, 24)                0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 1)                 25        \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 1)                 4         \n",
      "_________________________________________________________________\n",
      "sigmoid (Activation)         (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 1,073\n",
      "Trainable params: 999\n",
      "Non-trainable params: 74\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('./cnn_20x12_keras_trained')   #load trained model\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52ac573e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hls4ml_converter(model_name, directory, io_type, ReuseFactor):\n",
    "    \n",
    "    config = hls4ml.utils.config_from_keras_model(model_name, granularity='model')\n",
    "    print(\"-----------------------------------\")\n",
    "    config = {'Model': {'Precision': 'ap_fixed<5,1>',\n",
    "              'ReuseFactor': ReuseFactor,\n",
    "              'Strategy': 'Latency'}}\n",
    "    print(\"Configuration\")\n",
    "    display(config)\n",
    "    print(\"-----------------------------------\")\n",
    "    hls_model = hls4ml.converters.convert_from_keras_model(model_name,\n",
    "                                                           hls_config=config,   #set configuration of model\n",
    "                                                           output_dir=directory,  #6bit model save\n",
    "                                                           fpga_part='xcku15p-ffva1760-2-e',     #What?\n",
    "                                                            clock_period=(1/.24),      #set clock period??\n",
    "                                                              io_type=io_type)     #\n",
    "    hls_model.compile()\n",
    "    return hls_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21beaf77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: conv1_input, layer type: Input\n",
      "Layer name: conv1, layer type: Conv2D\n",
      "  -> Activation (linear), layer name: conv1\n",
      "Layer name: max_pooling2d, layer type: MaxPooling2D\n",
      "Layer name: batch_normalization, layer type: BatchNormalization\n",
      "Layer name: relu_c1, layer type: Activation\n",
      "Layer name: conv2, layer type: Conv2D\n",
      "  -> Activation (linear), layer name: conv2\n",
      "Layer name: max_pooling2d_1, layer type: MaxPooling2D\n",
      "Layer name: batch_normalization_1, layer type: BatchNormalization\n",
      "Layer name: relu_c2, layer type: Activation\n",
      "Layer name: dense1, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense1\n",
      "Layer name: batch_normalization_2, layer type: BatchNormalization\n",
      "Layer name: relu1, layer type: Activation\n",
      "Layer name: output, layer type: Dense\n",
      "  -> Activation (linear), layer name: output\n",
      "Layer name: batch_normalization_3, layer type: BatchNormalization\n",
      "Layer name: sigmoid, layer type: Activation\n",
      "-----------------------------------\n",
      "Configuration\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Model': {'Precision': 'ap_fixed<5,1>',\n",
       "  'ReuseFactor': 1,\n",
       "  'Strategy': 'Latency'}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: conv1_input, layer type: InputLayer, current shape: [[None, 20, 12, 1]]\n",
      "Layer name: conv1, layer type: Conv2D, current shape: [[None, 20, 12, 1]]\n",
      "Layer name: max_pooling2d, layer type: MaxPooling2D, current shape: [[None, 18, 10, 4]]\n",
      "Layer name: batch_normalization, layer type: BatchNormalization, current shape: [[None, 9, 5, 4]]\n",
      "Layer name: relu_c1, layer type: Activation, current shape: [[None, 9, 5, 4]]\n",
      "Layer name: conv2, layer type: Conv2D, current shape: [[None, 9, 5, 4]]\n",
      "Layer name: max_pooling2d_1, layer type: MaxPooling2D, current shape: [[None, 7, 3, 8]]\n",
      "Layer name: batch_normalization_1, layer type: BatchNormalization, current shape: [[None, 3, 1, 8]]\n",
      "Layer name: relu_c2, layer type: Activation, current shape: [[None, 3, 1, 8]]\n",
      "Layer name: dense1, layer type: Dense, current shape: [[None, 3, 1, 8]]\n",
      "Layer name: batch_normalization_2, layer type: BatchNormalization, current shape: [[None, 24]]\n",
      "Layer name: relu1, layer type: Activation, current shape: [[None, 24]]\n",
      "Layer name: output, layer type: Dense, current shape: [[None, 24]]\n",
      "Layer name: batch_normalization_3, layer type: BatchNormalization, current shape: [[None, 1]]\n",
      "Layer name: sigmoid, layer type: Activation, current shape: [[None, 1]]\n",
      "Creating HLS model\n",
      "Writing HLS project\n",
      "Done\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: conv1_input, layer type: Input\n",
      "Layer name: conv1, layer type: Conv2D\n",
      "  -> Activation (linear), layer name: conv1\n",
      "Layer name: max_pooling2d, layer type: MaxPooling2D\n",
      "Layer name: batch_normalization, layer type: BatchNormalization\n",
      "Layer name: relu_c1, layer type: Activation\n",
      "Layer name: conv2, layer type: Conv2D\n",
      "  -> Activation (linear), layer name: conv2\n",
      "Layer name: max_pooling2d_1, layer type: MaxPooling2D\n",
      "Layer name: batch_normalization_1, layer type: BatchNormalization\n",
      "Layer name: relu_c2, layer type: Activation\n",
      "Layer name: dense1, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense1\n",
      "Layer name: batch_normalization_2, layer type: BatchNormalization\n",
      "Layer name: relu1, layer type: Activation\n",
      "Layer name: output, layer type: Dense\n",
      "  -> Activation (linear), layer name: output\n",
      "Layer name: batch_normalization_3, layer type: BatchNormalization\n",
      "Layer name: sigmoid, layer type: Activation\n",
      "-----------------------------------\n",
      "Configuration\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Model': {'Precision': 'ap_fixed<5,1>',\n",
       "  'ReuseFactor': 1,\n",
       "  'Strategy': 'Latency'}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: conv1_input, layer type: InputLayer, current shape: [[None, 20, 12, 1]]\n",
      "Layer name: conv1, layer type: Conv2D, current shape: [[None, 20, 12, 1]]\n",
      "Layer name: max_pooling2d, layer type: MaxPooling2D, current shape: [[None, 18, 10, 4]]\n",
      "Layer name: batch_normalization, layer type: BatchNormalization, current shape: [[None, 9, 5, 4]]\n",
      "Layer name: relu_c1, layer type: Activation, current shape: [[None, 9, 5, 4]]\n",
      "Layer name: conv2, layer type: Conv2D, current shape: [[None, 9, 5, 4]]\n",
      "Layer name: max_pooling2d_1, layer type: MaxPooling2D, current shape: [[None, 7, 3, 8]]\n",
      "Layer name: batch_normalization_1, layer type: BatchNormalization, current shape: [[None, 3, 1, 8]]\n",
      "Layer name: relu_c2, layer type: Activation, current shape: [[None, 3, 1, 8]]\n",
      "Layer name: dense1, layer type: Dense, current shape: [[None, 3, 1, 8]]\n",
      "Layer name: batch_normalization_2, layer type: BatchNormalization, current shape: [[None, 24]]\n",
      "Layer name: relu1, layer type: Activation, current shape: [[None, 24]]\n",
      "Layer name: output, layer type: Dense, current shape: [[None, 24]]\n",
      "Layer name: batch_normalization_3, layer type: BatchNormalization, current shape: [[None, 1]]\n",
      "Layer name: sigmoid, layer type: Activation, current shape: [[None, 1]]\n",
      "Creating HLS model\n",
      "Writing HLS project\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "#hls_io_stream = hls4ml_converter(model, './hls_parameters/io_stream/hls4ml_prj', io_type='io_stream')\n",
    "hls_io_parallel = hls4ml_converter(model, './hls_parameters/io_parallel/hls4ml_prj', io_type='io_parallel')\n",
    "#hls_io_serial = hls4ml_converter(model, './hls_parameters/io_serial/hls4ml_prj', io_type='io_serial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff9dfaba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthesis report not found.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hls_io_stream.build(csim=False)\n",
    "#hls_io_parallel.build(csim=False)\n",
    "#hls_io_serial.build(csim=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b100692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 solution(s) in ./hls_parameters/io_parallel/hls4ml_prj/myproject_prj.\n",
      "Reports for solution \"solution1\":\n",
      "\n",
      "C simulation report not found.\n",
      "Synthesis report not found.\n",
      "Co-simulation report not found.\n"
     ]
    }
   ],
   "source": [
    "hls4ml.report.read_vivado_report('./hls_parameters/io_parallel/hls4ml_prj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8bdc22c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ReuseFactors = [256, 512, 1024, 2048, 4096]\n",
    "\n",
    "a = './hls_parameters/io_parallel'\n",
    "\n",
    "directories = []\n",
    "\n",
    "def remove_spaces(string):\n",
    "    a = string.replace(\",\", \"_\")\n",
    "    return a.replace(\" \", \"\")\n",
    "\n",
    "for r in ReuseFactors:\n",
    "    b = '_r'\n",
    "    b += str(r) \n",
    "    directories.append(a + b + '/hls4ml_prj')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fc4f264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: conv1_input, layer type: Input\n",
      "Layer name: conv1, layer type: Conv2D\n",
      "  -> Activation (linear), layer name: conv1\n",
      "Layer name: max_pooling2d, layer type: MaxPooling2D\n",
      "Layer name: batch_normalization, layer type: BatchNormalization\n",
      "Layer name: relu_c1, layer type: Activation\n",
      "Layer name: conv2, layer type: Conv2D\n",
      "  -> Activation (linear), layer name: conv2\n",
      "Layer name: max_pooling2d_1, layer type: MaxPooling2D\n",
      "Layer name: batch_normalization_1, layer type: BatchNormalization\n",
      "Layer name: relu_c2, layer type: Activation\n",
      "Layer name: dense1, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense1\n",
      "Layer name: batch_normalization_2, layer type: BatchNormalization\n",
      "Layer name: relu1, layer type: Activation\n",
      "Layer name: output, layer type: Dense\n",
      "  -> Activation (linear), layer name: output\n",
      "Layer name: batch_normalization_3, layer type: BatchNormalization\n",
      "Layer name: sigmoid, layer type: Activation\n",
      "-----------------------------------\n",
      "Configuration\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Model': {'Precision': 'ap_fixed<5,1>',\n",
       "  'ReuseFactor': 256,\n",
       "  'Strategy': 'Latency'}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: conv1_input, layer type: InputLayer, current shape: [[None, 20, 12, 1]]\n",
      "Layer name: conv1, layer type: Conv2D, current shape: [[None, 20, 12, 1]]\n",
      "Layer name: max_pooling2d, layer type: MaxPooling2D, current shape: [[None, 18, 10, 4]]\n",
      "Layer name: batch_normalization, layer type: BatchNormalization, current shape: [[None, 9, 5, 4]]\n",
      "Layer name: relu_c1, layer type: Activation, current shape: [[None, 9, 5, 4]]\n",
      "Layer name: conv2, layer type: Conv2D, current shape: [[None, 9, 5, 4]]\n",
      "Layer name: max_pooling2d_1, layer type: MaxPooling2D, current shape: [[None, 7, 3, 8]]\n",
      "Layer name: batch_normalization_1, layer type: BatchNormalization, current shape: [[None, 3, 1, 8]]\n",
      "Layer name: relu_c2, layer type: Activation, current shape: [[None, 3, 1, 8]]\n",
      "Layer name: dense1, layer type: Dense, current shape: [[None, 3, 1, 8]]\n",
      "Layer name: batch_normalization_2, layer type: BatchNormalization, current shape: [[None, 24]]\n",
      "Layer name: relu1, layer type: Activation, current shape: [[None, 24]]\n",
      "Layer name: output, layer type: Dense, current shape: [[None, 24]]\n",
      "Layer name: batch_normalization_3, layer type: BatchNormalization, current shape: [[None, 1]]\n",
      "Layer name: sigmoid, layer type: Activation, current shape: [[None, 1]]\n",
      "Creating HLS model\n",
      "Writing HLS project\n",
      "Done\n",
      "Synthesis report not found.\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: conv1_input, layer type: Input\n",
      "Layer name: conv1, layer type: Conv2D\n",
      "  -> Activation (linear), layer name: conv1\n",
      "Layer name: max_pooling2d, layer type: MaxPooling2D\n",
      "Layer name: batch_normalization, layer type: BatchNormalization\n",
      "Layer name: relu_c1, layer type: Activation\n",
      "Layer name: conv2, layer type: Conv2D\n",
      "  -> Activation (linear), layer name: conv2\n",
      "Layer name: max_pooling2d_1, layer type: MaxPooling2D\n",
      "Layer name: batch_normalization_1, layer type: BatchNormalization\n",
      "Layer name: relu_c2, layer type: Activation\n",
      "Layer name: dense1, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense1\n",
      "Layer name: batch_normalization_2, layer type: BatchNormalization\n",
      "Layer name: relu1, layer type: Activation\n",
      "Layer name: output, layer type: Dense\n",
      "  -> Activation (linear), layer name: output\n",
      "Layer name: batch_normalization_3, layer type: BatchNormalization\n",
      "Layer name: sigmoid, layer type: Activation\n",
      "-----------------------------------\n",
      "Configuration\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Model': {'Precision': 'ap_fixed<5,1>',\n",
       "  'ReuseFactor': 512,\n",
       "  'Strategy': 'Latency'}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: conv1_input, layer type: InputLayer, current shape: [[None, 20, 12, 1]]\n",
      "Layer name: conv1, layer type: Conv2D, current shape: [[None, 20, 12, 1]]\n",
      "Layer name: max_pooling2d, layer type: MaxPooling2D, current shape: [[None, 18, 10, 4]]\n",
      "Layer name: batch_normalization, layer type: BatchNormalization, current shape: [[None, 9, 5, 4]]\n",
      "Layer name: relu_c1, layer type: Activation, current shape: [[None, 9, 5, 4]]\n",
      "Layer name: conv2, layer type: Conv2D, current shape: [[None, 9, 5, 4]]\n",
      "Layer name: max_pooling2d_1, layer type: MaxPooling2D, current shape: [[None, 7, 3, 8]]\n",
      "Layer name: batch_normalization_1, layer type: BatchNormalization, current shape: [[None, 3, 1, 8]]\n",
      "Layer name: relu_c2, layer type: Activation, current shape: [[None, 3, 1, 8]]\n",
      "Layer name: dense1, layer type: Dense, current shape: [[None, 3, 1, 8]]\n",
      "Layer name: batch_normalization_2, layer type: BatchNormalization, current shape: [[None, 24]]\n",
      "Layer name: relu1, layer type: Activation, current shape: [[None, 24]]\n",
      "Layer name: output, layer type: Dense, current shape: [[None, 24]]\n",
      "Layer name: batch_normalization_3, layer type: BatchNormalization, current shape: [[None, 1]]\n",
      "Layer name: sigmoid, layer type: Activation, current shape: [[None, 1]]\n",
      "Creating HLS model\n",
      "Writing HLS project\n",
      "Done\n",
      "Synthesis report not found.\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: conv1_input, layer type: Input\n",
      "Layer name: conv1, layer type: Conv2D\n",
      "  -> Activation (linear), layer name: conv1\n",
      "Layer name: max_pooling2d, layer type: MaxPooling2D\n",
      "Layer name: batch_normalization, layer type: BatchNormalization\n",
      "Layer name: relu_c1, layer type: Activation\n",
      "Layer name: conv2, layer type: Conv2D\n",
      "  -> Activation (linear), layer name: conv2\n",
      "Layer name: max_pooling2d_1, layer type: MaxPooling2D\n",
      "Layer name: batch_normalization_1, layer type: BatchNormalization\n",
      "Layer name: relu_c2, layer type: Activation\n",
      "Layer name: dense1, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense1\n",
      "Layer name: batch_normalization_2, layer type: BatchNormalization\n",
      "Layer name: relu1, layer type: Activation\n",
      "Layer name: output, layer type: Dense\n",
      "  -> Activation (linear), layer name: output\n",
      "Layer name: batch_normalization_3, layer type: BatchNormalization\n",
      "Layer name: sigmoid, layer type: Activation\n",
      "-----------------------------------\n",
      "Configuration\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Model': {'Precision': 'ap_fixed<5,1>',\n",
       "  'ReuseFactor': 1024,\n",
       "  'Strategy': 'Latency'}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: conv1_input, layer type: InputLayer, current shape: [[None, 20, 12, 1]]\n",
      "Layer name: conv1, layer type: Conv2D, current shape: [[None, 20, 12, 1]]\n",
      "Layer name: max_pooling2d, layer type: MaxPooling2D, current shape: [[None, 18, 10, 4]]\n",
      "Layer name: batch_normalization, layer type: BatchNormalization, current shape: [[None, 9, 5, 4]]\n",
      "Layer name: relu_c1, layer type: Activation, current shape: [[None, 9, 5, 4]]\n",
      "Layer name: conv2, layer type: Conv2D, current shape: [[None, 9, 5, 4]]\n",
      "Layer name: max_pooling2d_1, layer type: MaxPooling2D, current shape: [[None, 7, 3, 8]]\n",
      "Layer name: batch_normalization_1, layer type: BatchNormalization, current shape: [[None, 3, 1, 8]]\n",
      "Layer name: relu_c2, layer type: Activation, current shape: [[None, 3, 1, 8]]\n",
      "Layer name: dense1, layer type: Dense, current shape: [[None, 3, 1, 8]]\n",
      "Layer name: batch_normalization_2, layer type: BatchNormalization, current shape: [[None, 24]]\n",
      "Layer name: relu1, layer type: Activation, current shape: [[None, 24]]\n",
      "Layer name: output, layer type: Dense, current shape: [[None, 24]]\n",
      "Layer name: batch_normalization_3, layer type: BatchNormalization, current shape: [[None, 1]]\n",
      "Layer name: sigmoid, layer type: Activation, current shape: [[None, 1]]\n",
      "Creating HLS model\n",
      "Writing HLS project\n",
      "Done\n",
      "Synthesis report not found.\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: conv1_input, layer type: Input\n",
      "Layer name: conv1, layer type: Conv2D\n",
      "  -> Activation (linear), layer name: conv1\n",
      "Layer name: max_pooling2d, layer type: MaxPooling2D\n",
      "Layer name: batch_normalization, layer type: BatchNormalization\n",
      "Layer name: relu_c1, layer type: Activation\n",
      "Layer name: conv2, layer type: Conv2D\n",
      "  -> Activation (linear), layer name: conv2\n",
      "Layer name: max_pooling2d_1, layer type: MaxPooling2D\n",
      "Layer name: batch_normalization_1, layer type: BatchNormalization\n",
      "Layer name: relu_c2, layer type: Activation\n",
      "Layer name: dense1, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense1\n",
      "Layer name: batch_normalization_2, layer type: BatchNormalization\n",
      "Layer name: relu1, layer type: Activation\n",
      "Layer name: output, layer type: Dense\n",
      "  -> Activation (linear), layer name: output\n",
      "Layer name: batch_normalization_3, layer type: BatchNormalization\n",
      "Layer name: sigmoid, layer type: Activation\n",
      "-----------------------------------\n",
      "Configuration\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Model': {'Precision': 'ap_fixed<5,1>',\n",
       "  'ReuseFactor': 2048,\n",
       "  'Strategy': 'Latency'}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: conv1_input, layer type: InputLayer, current shape: [[None, 20, 12, 1]]\n",
      "Layer name: conv1, layer type: Conv2D, current shape: [[None, 20, 12, 1]]\n",
      "Layer name: max_pooling2d, layer type: MaxPooling2D, current shape: [[None, 18, 10, 4]]\n",
      "Layer name: batch_normalization, layer type: BatchNormalization, current shape: [[None, 9, 5, 4]]\n",
      "Layer name: relu_c1, layer type: Activation, current shape: [[None, 9, 5, 4]]\n",
      "Layer name: conv2, layer type: Conv2D, current shape: [[None, 9, 5, 4]]\n",
      "Layer name: max_pooling2d_1, layer type: MaxPooling2D, current shape: [[None, 7, 3, 8]]\n",
      "Layer name: batch_normalization_1, layer type: BatchNormalization, current shape: [[None, 3, 1, 8]]\n",
      "Layer name: relu_c2, layer type: Activation, current shape: [[None, 3, 1, 8]]\n",
      "Layer name: dense1, layer type: Dense, current shape: [[None, 3, 1, 8]]\n",
      "Layer name: batch_normalization_2, layer type: BatchNormalization, current shape: [[None, 24]]\n",
      "Layer name: relu1, layer type: Activation, current shape: [[None, 24]]\n",
      "Layer name: output, layer type: Dense, current shape: [[None, 24]]\n",
      "Layer name: batch_normalization_3, layer type: BatchNormalization, current shape: [[None, 1]]\n",
      "Layer name: sigmoid, layer type: Activation, current shape: [[None, 1]]\n",
      "Creating HLS model\n",
      "Writing HLS project\n",
      "Done\n",
      "Synthesis report not found.\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: conv1_input, layer type: Input\n",
      "Layer name: conv1, layer type: Conv2D\n",
      "  -> Activation (linear), layer name: conv1\n",
      "Layer name: max_pooling2d, layer type: MaxPooling2D\n",
      "Layer name: batch_normalization, layer type: BatchNormalization\n",
      "Layer name: relu_c1, layer type: Activation\n",
      "Layer name: conv2, layer type: Conv2D\n",
      "  -> Activation (linear), layer name: conv2\n",
      "Layer name: max_pooling2d_1, layer type: MaxPooling2D\n",
      "Layer name: batch_normalization_1, layer type: BatchNormalization\n",
      "Layer name: relu_c2, layer type: Activation\n",
      "Layer name: dense1, layer type: Dense\n",
      "  -> Activation (linear), layer name: dense1\n",
      "Layer name: batch_normalization_2, layer type: BatchNormalization\n",
      "Layer name: relu1, layer type: Activation\n",
      "Layer name: output, layer type: Dense\n",
      "  -> Activation (linear), layer name: output\n",
      "Layer name: batch_normalization_3, layer type: BatchNormalization\n",
      "Layer name: sigmoid, layer type: Activation\n",
      "-----------------------------------\n",
      "Configuration\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Model': {'Precision': 'ap_fixed<5,1>',\n",
       "  'ReuseFactor': 4096,\n",
       "  'Strategy': 'Latency'}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: conv1_input, layer type: InputLayer, current shape: [[None, 20, 12, 1]]\n",
      "Layer name: conv1, layer type: Conv2D, current shape: [[None, 20, 12, 1]]\n",
      "Layer name: max_pooling2d, layer type: MaxPooling2D, current shape: [[None, 18, 10, 4]]\n",
      "Layer name: batch_normalization, layer type: BatchNormalization, current shape: [[None, 9, 5, 4]]\n",
      "Layer name: relu_c1, layer type: Activation, current shape: [[None, 9, 5, 4]]\n",
      "Layer name: conv2, layer type: Conv2D, current shape: [[None, 9, 5, 4]]\n",
      "Layer name: max_pooling2d_1, layer type: MaxPooling2D, current shape: [[None, 7, 3, 8]]\n",
      "Layer name: batch_normalization_1, layer type: BatchNormalization, current shape: [[None, 3, 1, 8]]\n",
      "Layer name: relu_c2, layer type: Activation, current shape: [[None, 3, 1, 8]]\n",
      "Layer name: dense1, layer type: Dense, current shape: [[None, 3, 1, 8]]\n",
      "Layer name: batch_normalization_2, layer type: BatchNormalization, current shape: [[None, 24]]\n",
      "Layer name: relu1, layer type: Activation, current shape: [[None, 24]]\n",
      "Layer name: output, layer type: Dense, current shape: [[None, 24]]\n",
      "Layer name: batch_normalization_3, layer type: BatchNormalization, current shape: [[None, 1]]\n",
      "Layer name: sigmoid, layer type: Activation, current shape: [[None, 1]]\n",
      "Creating HLS model\n",
      "Writing HLS project\n",
      "Done\n",
      "Synthesis report not found.\n"
     ]
    }
   ],
   "source": [
    "for i,r in enumerate(ReuseFactors):\n",
    "    hls_io_parallel = hls4ml_converter(model, directories[i], io_type='io_parallel', ReuseFactor=r)\n",
    "    hls_io_parallel.build(csim=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe1876d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 solution(s) in ./hls_parameters/io_parallel_r256/hls4ml_prj/myproject_prj.\n",
      "Reports for solution \"solution1\":\n",
      "\n",
      "C simulation report not found.\n",
      "Synthesis report not found.\n",
      "Co-simulation report not found.\n",
      "Found 1 solution(s) in ./hls_parameters/io_parallel_r512/hls4ml_prj/myproject_prj.\n",
      "Reports for solution \"solution1\":\n",
      "\n",
      "C simulation report not found.\n",
      "Synthesis report not found.\n",
      "Co-simulation report not found.\n",
      "Found 1 solution(s) in ./hls_parameters/io_parallel_r1024/hls4ml_prj/myproject_prj.\n",
      "Reports for solution \"solution1\":\n",
      "\n",
      "C simulation report not found.\n",
      "Synthesis report not found.\n",
      "Co-simulation report not found.\n",
      "Found 1 solution(s) in ./hls_parameters/io_parallel_r2048/hls4ml_prj/myproject_prj.\n",
      "Reports for solution \"solution1\":\n",
      "\n",
      "C simulation report not found.\n",
      "Synthesis report not found.\n",
      "Co-simulation report not found.\n",
      "Found 1 solution(s) in ./hls_parameters/io_parallel_r4096/hls4ml_prj/myproject_prj.\n",
      "Reports for solution \"solution1\":\n",
      "\n",
      "C simulation report not found.\n",
      "Synthesis report not found.\n",
      "Co-simulation report not found.\n"
     ]
    }
   ],
   "source": [
    "for d in directories:\n",
    "    hls4ml.report.read_vivado_report(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5745585c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
