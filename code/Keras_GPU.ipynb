{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Neural Networks on DICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook runs through two quick examples of training neural networks on a) single machine with six GPU cores and b) scaling out to a cluster of GPU nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note option b is still under development at the time of writing but please get in touch if your workflow requires these resources. Alternatively, CERN staff can request cloud compute vouchers for either Amazon Web Services (AWS) or Google Cloud Platfrom (GCP) to utilise Tensor Processing Units (TPUs) -  a snow ticket for resource evaluation to be sent to the IT depeartment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up either a python virtual or conda environment in the usual way, if you're creating a new environment from scratch, make sure your `environment.yml` file contains the `-tensorflow-gpu` and `keras-tuner` dependencies. If you want to add this dependency to an exisiting environment, then execute: `conda install -c anaconda tensorflow-gpu && conda install -c conda-forge keras-tuner`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Connect to worker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The IP address of node with GPU cores: `10.129.5.43`\n",
    "- now execute: `ssh <my_dice_username>@10.129.5.43`\n",
    "- this machine still mounts all storage systems available on DICE (big files are best kept in the `/scratch/$USER` directory.\n",
    "\n",
    "- Note: these machine don't yet support dynamic allocation, so check they're not currently in use by others. To monitor the NVIDIA GPU cards, execute: `nvidia-smi` or to constantly track usage `watch -d -n 0.5 nvidia-smi`\n",
    "\n",
    "- if you want to kill a running GPU job, execute: `kill -9 <PID>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example training script.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dependencies \n",
    "\n",
    "import tensorflow as tf    \n",
    "from tensorflow.keras.layers import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Memory management, not too important but must include\n",
    "\n",
    "all_devices = len(tf.config.list_physical_devices('GPU'))\n",
    "print(\"Num GPUs Available: \", all_devices)\n",
    "physical_devices=tf.config.list_physical_devices('GPU')\n",
    "gpus= tf.config.experimental.list_physical_devices('GPU')\n",
    "for i in range(0,all_devices+1):\n",
    "    tf.config.experimental.set_memory_growth(gpus[i], True)\n",
    "\n",
    "mirrored_strategy = tf.distribute.MirroredStrategy(devices=[f\"/GPU:{GPU_id}\" for GPU_id in range (0,6)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define some model, e.g. two convolutional feature extraction layers \n",
    "#and a feed forward classification layer\n",
    "#we will also perform hyperparameter search\n",
    "\n",
    "def create_model(hp):\n",
    "    with mirrored_strategy.scope():\n",
    "        model = tf.keras.Sequential()\n",
    "        for i in range (1, hp.Int(\"conv_layers\",3,4)):\n",
    "            if i == 1:\n",
    "                model.add(Conv2D(4, kernel_size=hp.Choice('kernel_size', values=[2,3,4]), input_shape=(20,12,1),  padding='same'))\n",
    "            else:\n",
    "                 model.add(Conv2D(8, kernel_size=hp.Choice('kernel_size', values=[2,3,4]), padding='same'))\n",
    "            model.add(MaxPooling2D((2,2), padding='same'))\n",
    "            model.add(BatchNormalization(axis=1))\n",
    "            model.add(Activation('relu'))\n",
    "\n",
    "        model.add(Flatten())\n",
    "\n",
    "        for j in range (1, hp.Int(\"FCN_layers\",3,4)):\n",
    "            model.add(Dense(128))\n",
    "            model.add(BatchNormalization(axis=1))\n",
    "            model.add(Activation('relu'))\n",
    "\n",
    "        model.add(Dense(1))\n",
    "        model.add(BatchNormalization(axis=1))\n",
    "        model.add(Activation('sigmoid'))\n",
    "\n",
    "        model.build(input_shape=(20,12,1))\n",
    "\n",
    "        opt = tf.keras.optimizers.SGD(\n",
    "            learning_rate=hp.Choice(\"lr\", values=[0.1,0.01])\n",
    "        )\n",
    "\n",
    "        model.compile(loss='binary_crossentropy', optimizer=opt, metrics=[\"accuracy\"])\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use Keras tuner to define hyperparameter search\n",
    "tuner=RandomSearch(create_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=5,\n",
    "    overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in your dataset\n",
    "data = np.load('/software/ys20884/training_data/data_hh4b_20x12_160000.npz')\n",
    "train_X= data['train_X']      \n",
    "train_y = data['train_y']   \n",
    "test_X = data['test_X']\n",
    "test_y = data['test_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#futher split into validation set\n",
    "#could also use k-fold cross validation\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(train_X, train_y, test_size=0.15, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(x_train, y_train, epochs=1,  validation_data=(x_valid,y_valid), workers=all_devices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print your best found parameters (good idea to also save these!)\n",
    "\n",
    "print(tuner.results_summary())\n",
    "\n",
    "bestHP = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(\"[INFO] optimal number of filters in conv_1 layer: {}\".format(\n",
    "bestHP.get(\"conv_layers\")))\n",
    "print(\"[INFO] optimal number of filters in conv_2 layer: {}\".format(\n",
    "bestHP.get(\"FNC_layesr\")))\n",
    "print(\"[INFO] optimal number of units in dense layer: {}\".format(\n",
    "bestHP.get(\"kernel_size\")))\n",
    "print(\"[INFO] optimal learning rate: {:.4f}\".format(\n",
    "bestHP.get(\"lr\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can now train with these optimal parameters:\n",
    "model = tuner.hypermodel.build(bestHP)\n",
    "history = model.fit(train_X, train_y, epochs=50, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Troubleshooting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If Tesnorflow is \"not seeing\" the CUDA workers (i.e. all_devices=0) then try exeuting the following commands:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `pip uninstall tensorflow`\n",
    "- `pip uninstall tensorflow-gpu`\n",
    "- `pip install --upgrade --force-reinstall tensorflow-gpu`\n",
    "- `export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B) Training on cluster w/ Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: backend still under development and will sometimes experiance\n",
    "RAM memory issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, SSHCluster\n",
    "import asyncio, asyncssh, sys\n",
    "cluster = SSHCluster(\n",
    "    [\"10.129.5.2\", \"deepthought.phy.bris.ac.uk\"],\n",
    "    connect_options={\"known_hosts\": None,\n",
    "                     \"username\": \"\",\n",
    "                     \"password\": \"\"},\n",
    "    scheduler_options={\"port\": 0, \"dashboard_address\": \":8797\"})\n",
    "    #worker_class=\"dask_cuda.CUDAWorker\")\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.scheduler_info()['services']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing.pool import ThreadPool\n",
    "import dask\n",
    "dask.config.set(pool=ThreadPool(1))\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasClassifier(build_fn=create_model, batch_size=2, epochs=1, kernel_size=(3,3), pool_size=(2,2), dropout=0.1, conv_layers=1, hidden_layers=1, FCN_dense=24, CNN_dense=4,lr=0.1, momentum=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'hidden_layers' : [3,4],\n",
    "    'conv_layers' : [3,4],\n",
    "    'lr': [0.1,0.01],\n",
    "    'momentum': [0.05],\n",
    "    'kernel_size': [(4,4),(3,3)],\n",
    "    'pool_size': [(2,2)],\n",
    "    'epochs': [10,20,50],\n",
    "    'FCN_dense': [64], \n",
    "    'batch_size': [32],\n",
    "    'dropout': [0.01, 0]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold_splits = 4\n",
    "grid = GridSearchCV(estimator=model,  \n",
    "                    return_train_score=True,\n",
    "                    cv=kfold_splits,\n",
    "                    param_grid=param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with joblib.parallel_backend('dask'):\n",
    "    grid.fit(train_X, train_y)\n",
    "    print(grid.best_params_)\n",
    "\n",
    "client.shutdown()\n",
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
